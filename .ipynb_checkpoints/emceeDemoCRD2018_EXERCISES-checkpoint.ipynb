{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Model Fitting in Python with emcee\n",
    "## *Computational Research Day 2018 Workshop*\n",
    "\n",
    "\n",
    "***\n",
    "By AM Geller and AA Miller <br/>\n",
    "April 10, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Today we will focus on a seemingly simple problem: fitting a straight line to data.\n",
    "\n",
    "Though we've all done this before, we've probably at some point oversimplified the analysis or at least not fully understood all the assumptions that went into our analysis. \n",
    "\n",
    "This workshop is inspired by the paper [Data Analysis Recipes: Fitting a Model to Data](https://arxiv.org/abs/1008.4686) by Hogg, Bovy, & Lang. If you haven't already read this paper, I ***highly*** recommend it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#For RISE slideshow\n",
    "#see options in https://github.com/bennylope/jupyter-python3-functional/tree/master/RISE/livereveal/reveal.js\n",
    "from traitlets.config.manager import BaseJSONConfigManager\n",
    "path = \"/Users/ageller/anaconda/envs/py36/etc/jupyter/nbconfig\"\n",
    "cm = BaseJSONConfigManager(config_dir=path)\n",
    "cm.update(\"livereveal\", {\n",
    "            \"theme\": \"simple\", #beige/blood/default/moon/night/serif/simple/sky/solarized\n",
    "            \"transition\": \"concave\", # default/cube/page/concave/zoom/linear/fade/none\n",
    "            \"start_slideshow_at\": \"beginning\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import emcee\n",
    "import corner\n",
    "\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(seed=2222)\n",
    "ncores = 6 # adjust this to match your machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The first thing we need is the data.  Let's start with a prepared data set from that paper.  Later we can move to a real data set (maybe your own).\n",
    "\n",
    "In this data set, you'll notice that each $\\left(x,y\\right)$ point has an uncertainty characterized by a guassian with variance $\\left(\\sigma_x^, \\sigma_y^2\\right)$, while there is also covariance between the $x$ and $y$ measurements described by a correlation coefficient $\\rho_{xy}$.  I found [this nice 1-page info sheet online](http://www.cs.utah.edu/~tch/CS4300/resources/refs/ErrorEllipses.pdf) if you want a further reference.\n",
    "\n",
    "The full uncertainty covariance matrix for each data point is given by:\n",
    "\n",
    "$$S_i = \\left[ {\\begin{array}{cc}\n",
    "        \\sigma_{xi}^2 & \\rho_{xyi}\\sigma_{xi}\\sigma_{yi} \\\\\n",
    "        \\rho_{xyi}\\sigma_{xi}\\sigma_{yi} & \\sigma_{yi}^2  \\\\\n",
    "        \\end{array}\n",
    "       }\n",
    "       \\right] \n",
    "       $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "obs_x = np.array([203., 58., 210., 202., 198., 158., \n",
    "                165., 201., 157., 131., 166., 160., \n",
    "                186., 125., 218., 146.])\n",
    "obs_y = np.array([495., 173., 479., 504., 510., 416., \n",
    "                393., 442., 317., 311., 400., 337., \n",
    "                423., 334., 533., 344.])\n",
    "sigma_x = np.array([5., 9., 4., 4., 11., 7., \n",
    "                    5., 5., 5., 6., 6., 5., \n",
    "                    9., 8., 6., 5.])\n",
    "sigma_y = np.array([21., 15., 27., 14., 30., 16., \n",
    "                    14., 25., 52., 16., 34., 31., \n",
    "                    42., 26., 16., 22.])\n",
    "rho_xy = np.array([-0.33, 0.67, -0.02, -0.05, -0.84, -0.69,\n",
    "                    0.30, -0.46, -0.03, 0.50, 0.73, -0.52, \n",
    "                    0.90, 0.40, -0.78, -0.56])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def plotData(ax, x, y, xerr, yerr, rhoxy, zorder = 10):\n",
    "    ax.scatter(x, y, color = 'black', zorder = 10)\n",
    "    for (xx, yy, sx, sy, rxy) in zip(x,y,xerr, yerr, rhoxy):\n",
    "        cov = np.array([[ sx**2, rxy*sx*sy],\n",
    "                        [rxy*sx*sy, sy**2.]])\n",
    "        w, v  = np.linalg.eig(cov)\n",
    "        theta = np.arctan(2. * rxy * sx * sy / (sx**2. - sy**2.))/2.\n",
    "        ax.add_artist(Ellipse((xx, yy), 2.*w[0]**0.5, 2.*w[1]**0.5, \\\n",
    "                              angle = np.rad2deg(theta),\\\n",
    "                              facecolor=\"none\", edgecolor=\"black\", zorder = zorder))\n",
    "        \n",
    "f, ax = plt.subplots(1, figsize = (5,5))\n",
    "plotData(ax, obs_x, obs_y, sigma_x, sigma_y, rho_xy)\n",
    "ax.set_xlim(0,300)\n",
    "ax.set_ylim(0,700)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, let's fit the line :\n",
    "\n",
    "$y = mx + b$.  \n",
    "\n",
    "How would you do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Probably the first attempt would be some kind of least squares optimization ... but ...\n",
    "\n",
    "A standard assumption of the least-squares method is that the independent variable (typically $x$) is measured with perfect precision. Thus, standard least squares cannot account for the uncertainties on both $x$ and $y$ (simultaneously).  As a demonstration of ***WHAT NOT TO DO*** (but is sometimes attempted), here's an example of a \"forward-reverse\" fitting procedure, where you first fit by ignoring $\\sigma_x$ then do the inverse and ignore $\\sigma_y$.  \n",
    "\n",
    "I repeat ***DO NOT DO THIS*** for a real publication.  This is only to illustrate a point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize = (5,5))\n",
    "plotData(ax, obs_x, obs_y, sigma_x, sigma_y, rho_xy)\n",
    "ax.set_xlim(0,300)\n",
    "ax.set_ylim(0,700)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "#\"abominable\" forwardâ€“reverse fitting procedure\n",
    "#DON'T DO THIS!\n",
    "#polyfit with uncertainties on y (\"forward\")\n",
    "p_xy, pcov = np.polyfit(obs_x, obs_y, 1, w = 1./sigma_y, cov = True)\n",
    "err_xy = np.sqrt(np.diag(pcov))\n",
    "p_xy_eval = np.poly1d(p_xy)\n",
    "print(\"The best-fit value for the ('forward') slope and intercept are: {:.4f} +/- {:.4f} and {:.4f} +/- {:.4f}\"\\\n",
    "      .format(p_xy[0], err_xy[0], p_xy[1], err_xy[1]))\n",
    "ax.plot([0,300],p_xy_eval([0,300]),color='C0')\n",
    "\n",
    "#polyfit with uncertainties on x (\"reverse\")\n",
    "p_yx, pcov = np.polyfit(obs_y, obs_x, 1, w = 1./sigma_x, cov = True)\n",
    "err_yx = np.sqrt(np.diag(pcov))\n",
    "p_yx_eval = np.poly1d(p_yx)\n",
    "inv = [1./p_yx[0], -p_yx[1]/p_yx[0]]\n",
    "inv_err = [inv[0]**2.*err_yx[0],  (p_yx[0]**-2 * err_yx[1]**2. + (p_yx[1]/p_yx[0]**2.)**2. * err_yx[0]**2.)**0.5]\n",
    "print(\"The best-fit value for the ('reverse') slope and intercept are: {:.4f} +/- {:.4f} and {:.4f} +/- {:.4f}\"\\\n",
    "      .format(inv[0], inv_err[0], inv[1], inv_err[1]))\n",
    "ax.plot(p_yx_eval([0.,700.]), [0.,700.],'--',color='C1')\n",
    "\n",
    "mFreq = p_xy[0]\n",
    "bFreq = p_xy[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Again ***DO NOT DO THIS IN A PUBLICATION***. \n",
    "\n",
    "It should be fairly obvious why you don't want to do this.  For instance, \n",
    "- The results (at least for the intercept) are not in agreement, given the uncertainties on the parameters\n",
    "- How would you combine these results to give some meaningful answer?\n",
    "- Do you actually trust the uncertainties on these parameters?  Are the parameter uncertainties actually symmetric?\n",
    "- Most importantly, neither method actually accounts for both uncertainties or their correlation coefficients ($\\rho_{xy}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Least-squares fitting reduces to some linear algebra. [See the Hogg, Bovy, & Lang paper referenced above for the derivation of these relations.]\n",
    "\n",
    "Below we demonstrate the same results as `np.polyfit()` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#a little bit more thorough and returns the same result as the \"forward\" fit\n",
    "Y = obs_y.reshape(-1,1)\n",
    "A = np.vstack((np.ones_like(obs_x), obs_x)).T\n",
    "C = np.diag(sigma_y**2)\n",
    "\n",
    "X = np.linalg.inv(A.transpose()@np.linalg.inv(C)@A) @ (A.transpose()@np.linalg.inv(C)@Y)\n",
    "\n",
    "best_fit = np.poly1d(X[::-1,0])\n",
    "print(\"The best-fit value for the slope and intercept are: {:.4f} and {:.4f}\".format(X[1][0], X[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "OK.  The naive first attempt is not appropriate for this problem.  That least squares fitting makes certain assumptions (e.g., that the $x$ uncertainties are negligible).  Let's relax those assumptions and frame the problem more generally.\n",
    "\n",
    "First, some statistics... (Some of you may enjoy this, others may not.  It's OK if you don't fully understand all of this today.  We will get to the `emcee` fitter very soon.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Minimizing $\\chi^2$\n",
    "\n",
    "A common procedure taught in undergraduate lab classes is to minimize the \"chi squared\" in order to identify the \"best-fit\" parameters $m$ and $b$: \n",
    "\n",
    "$$ \\chi^2 = \\Sigma_{i = 1}^{N} \\frac{\\left[y_i - f(x_i)\\right]^2}{\\sigma_{y_i}^2},$$\n",
    "\n",
    "where $f(x) = mx + b$ is the model for the data. \n",
    "\n",
    "A least-squares fit, such as the ones used above, minimizes the value of $\\chi^2$, which in turn provides a \"best-fit\" estimate of $m$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Maximizing the Likelihood\n",
    "\n",
    "Alternatively, we could consider a generative model, in which case the probability of any observation $y_i$ is dependent on $x_i$, our model $f(x)$, and some scatter associated with the measurement itself $\\sigma_{yi}$:\n",
    "\n",
    "$$ p\\left(y_i\\, | \\, x_i, \\sigma_{yi}, f(x_i) \\right) = \\frac{1}{\\sqrt{2\\pi\\sigma_{y_i}^2}}\\,\\exp\\left(-\\frac{\\left[y_i - f(x_i)\\right]^2}{2\\,\\sigma_{y_i}^2} \\right).$$\n",
    "\n",
    "An alternative to $\\chi^2$ minimization is to maximize the probability of the observed data given the model, which is to say we want to maximize the *likelihood of the observed data given the model*. We can write the likelihood function $\\mathscr{L}$ as the product of the conditional probability of every observation:\n",
    "\n",
    "$$ \\mathscr{L} = \\prod_{i=1}^{N} p\\left(y_i\\, |\\, x_i, \\sigma_i, f(x_i) \\right)$$\n",
    "\n",
    "This product is often difficult to evaluate, so we often take the logarithm of the likelihood, and maximize that. In this case we get:\n",
    "\n",
    "$$ \\ln \\mathscr{L} = K - \\sum_{i=1}^{N} \\frac{\\left[y_i - f(x_i) \\right]^2.}{2\\sigma_{yi}^2} \n",
    "   = K - \\frac{1}{2}\\chi^2 $$\n",
    "   \n",
    "where $K$ is some constant. As you can see, maximizing the likelihood (in this case) is equivalent to minimizing $\\chi^2$.\n",
    "\n",
    "Note: an important assumption in all of this is that the uncertainties, $\\left(\\sigma_x, \\sigma_y\\right)$, are Gaussian.  This is usually a reasonable assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When maximizing the likelihood we can get a \"pretty good\" estimate of the model parameters, $m$ and $b$. This estimate is only \"pretty good\" because Thomas Bayes is rolling in his grave. Bayes' theorem tells us about conditional probability distributions:\n",
    "\n",
    "$$ p\\left(\\theta\\, |\\, X, I\\right) = \\frac{p\\left(X\\, |\\, \\theta, I\\right) p\\left(\\theta\\, |\\, I\\right)}{p\\left(X\\, |\\, I\\right)} $$\n",
    "\n",
    "In words, we want to know the (*posterior*) probability distribution for the parameters $\\theta$, which $= (m,b)$ in this case, given the data $X$ and any prior knowledge $I$, $p\\left(\\theta\\, |\\, X, I\\right)$.  \n",
    "\n",
    "$p\\left(X\\, |\\, \\theta, I\\right)$ is the *likelihood*, and has been defined above.  \n",
    "\n",
    "$p\\left(\\theta\\, |\\, I\\right)$ is the \"*prior*\" probability distribution for the parameters, that contains all of our knowledge of what those parameters should be (this can come from physical models or previous experiments).  \n",
    "\n",
    "The denominator can be thought of as a normalization constant that most people ignore (and we will also ignore it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ok.  That's very nice.  \n",
    "\n",
    "But how do we actually use all this math??!!  \n",
    "\n",
    "And, hey, you said this was a workshop about emcee!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# compute the log likelihood\n",
    "def lnlike(theta, x, y, yerr):\n",
    "    m, b = theta\n",
    "    model = m * x + b\n",
    "    ln_l = np.sum( np.log( 1./np.sqrt(2*np.pi *sigma_y**2.) * np.exp(-0.5*((y - model)/sigma_y)**2) ) )\n",
    "    \n",
    "    return ln_l.squeeze()\n",
    "\n",
    "# compute the log prior\n",
    "def lnprior(theta):\n",
    "    m, b = theta\n",
    "    if ( (0 < m < 10) and (-200 < b < 200) ):\n",
    "        return 0.0\n",
    "    return -np.inf\n",
    "\n",
    "# compute the log of the likelihood multiplied by the prior\n",
    "def lnprob(theta, x, y, yerr):\n",
    "    lnp = lnprior(theta)\n",
    "    lnl = lnlike(theta,  x, y, yerr)\n",
    "    if (not np.isfinite(lnp) or not np.isfinite(lnl)):\n",
    "        return -np.inf\n",
    "    return lnp + lnl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, as a check, let's try to maximize this likelihood with scipy and confirm that we get the same results as the least-squares procedure described above (actually we'll minimize the negative log likelihood as this is an easier computational task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def ilnlike(theta, x, y, yerr):\n",
    "    lnl = -lnlike(theta, x, y, yerr)\n",
    "    return lnl.squeeze()\n",
    "\n",
    "guess0 = np.array([2.*np.random.random() * mFreq, 2.*np.random.random() * bFreq])\n",
    "scFit = minimize(ilnlike, guess0, args=(obs_x, obs_y, sigma_y))\n",
    "print(scFit.message)\n",
    "print(\"The initial guess for the slope and intercept are: {:.4f} and {:.4f}\"\\\n",
    "      .format(guess0[0], guess0[1]))\n",
    "print(\"The best-fit value for the slope and intercept are: {:.4f} and {:.4f}\".format(scFit.x[0], scFit.x[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## *emcee*\n",
    "\n",
    "Now, let's use *[emcee](http://dfm.io/emcee/current/)*, a pure-Python implementation of [Goodman & Weareâ€™s Affine Invariant Markov chain Monte Carlo (MCMC) Ensemble sampler](https://msp.org/camcos/2010/5-1/p04.xhtml) written by [Dan Foreman-Mackey](http://dfm.io/).\n",
    "\n",
    "We've already spent enough time on background, so I will let you read about MCMC on your own (e.g., [here's the wikipedia entry](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo)).  You may also want to read [this journal article about emcee](https://arxiv.org/abs/1202.3665).\n",
    "\n",
    "In short, we can use *emcee* to sample the posterior distribution that is defined by a Bayesian likelihood and priors (using MCMC).  *emcee* will use a number of \"walkers\" to explore this parameter space, each sampling around the maximum of the likelihood function (while accounting for the priors).  The combination of the paths that each of these walkers take, i.e. their \"chains\", define the posterior distribution, and provide us with probability distributions for each of the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "nwalkers = 100\n",
    "nfac = [1e-2, 1e-2]\n",
    "ndim = len(guess0)\n",
    "pos = [guess0 + nfac * np.random.randn(ndim) for i in range(nwalkers)]\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, args=(obs_x, obs_y, sigma_y), threads = ncores)\n",
    "\n",
    "nsamples = 2000\n",
    "foo = sampler.run_mcmc(pos, nsamples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And now let's explore the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# set a \"burn-in\" limit\n",
    "nburn = 200\n",
    "m_samples = np.array(sampler.chain[:,nburn:,0]).flatten()\n",
    "b_samples = np.array(sampler.chain[:,nburn:,1]).flatten()\n",
    "\n",
    "#print the results taking 1-sigma confidence intervals\n",
    "samples = np.vstack([m_samples, b_samples]).T\n",
    "m_mcmc, b_mcmc = map(lambda v: (v[1], v[2]-v[1], v[1]-v[0]), zip(*np.percentile(samples, [16, 50, 84], axis=0)))\n",
    "print(\"emcee results with 1-sigma uncertainties\\n \\\n",
    "      m = {:.4f} +{:.4f} -{:.4f}\\n \\\n",
    "      b = {:.4f} +{:.4f} -{:.4f}\\n\" \\\n",
    "      .format(m_mcmc[0], m_mcmc[1], m_mcmc[2], b_mcmc[0], b_mcmc[1], b_mcmc[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#some function to make plots (will re-use them later)\n",
    "#to plot the chains\n",
    "def plotChains(sampler, nburn, paramsNames):\n",
    "    Nparams = len(paramsNames)\n",
    "    fig, ax = plt.subplots(Nparams,1, figsize = (8,2*Nparams), sharex = True)\n",
    "    fig.subplots_adjust(hspace = 0)\n",
    "    ax[0].set_title('Chains')\n",
    "    xplot = range(len(sampler.chain[0,:,0]))\n",
    "\n",
    "    for i,p in enumerate(paramsNames):\n",
    "        for w in range(sampler.chain.shape[0]):\n",
    "            ax[i].plot(xplot[:nburn], sampler.chain[w,:nburn,i], color=\"0.5\", alpha = 0.4, lw = 0.7, zorder = 1)\n",
    "            ax[i].plot(xplot[nburn:], sampler.chain[w,nburn:,i], color=\"k\", alpha = 0.4, lw = 0.7, zorder = 1)\n",
    "            \n",
    "            ax[i].set_ylabel(p)\n",
    "            \n",
    "    return ax\n",
    "\n",
    "paramsNames = ['m','b']\n",
    "\n",
    "axC = plotChains(sampler, nburn, paramsNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#corner plot\n",
    "def makeCorner(sampler, nburn, paramsNames, quantiles=[0.16, 0.5, 0.84]):\n",
    "    samples = sampler.chain[:, nburn:, :].reshape((-1, len(paramsNames)))\n",
    "    f = corner.corner(samples, labels = paramsNames, quantiles = quantiles)\n",
    "\n",
    "makeCorner(sampler, nburn, paramsNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Adam's plots\n",
    "def prettyPlot(x, y, xerr, yerr, rhoxy, m_samples, b_samples, m, b, nburn, ndraw = 20, extent = None):\n",
    "    fig, ax = plt.subplots(1,2, figsize = (8,4))\n",
    "    ax[0].set_xlabel(\"m\")\n",
    "    ax[0].set_ylabel(\"b\")\n",
    "    ax[1].set_xlabel(\"x\")    \n",
    "    ax[1].set_ylabel(\"y\")\n",
    "\n",
    "    ax[0].hexbin(m_samples[nburn:], b_samples[nburn:], gridsize = 250,  mincnt = 1, bins = \"log\", extent = extent)\n",
    "\n",
    "    plotData(ax[1], x, y, xerr, yerr, rhoxy)\n",
    "    ax[1].plot([0,300], [b + 0*m, b + 300*m], lw = 2, zorder = 2)\n",
    "    if (ndraw > 0):\n",
    "        for rand_draw in np.random.randint(nburn, len(m_samples), ndraw):\n",
    "            m_draw = m_samples[rand_draw]\n",
    "            b_draw = b_samples[rand_draw]\n",
    "            ax[1].plot([-1e10,1e10], [b_draw + -1e10*m_draw, b_draw + 1e10*m_draw],\n",
    "                     color = \"0.5\", alpha = 0.4, lw = 0.7, zorder = 1)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "axA = prettyPlot(obs_x, obs_y, sigma_x, sigma_y, rho_xy, m_samples, b_samples, m_mcmc[0], b_mcmc[0], nburn, ndraw = 20)\n",
    "axA[1].set_xlim(0,300)\n",
    "axA[1].set_ylim(0,700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "OK.  But... didn't we just get the same result as before? \n",
    "\n",
    "And, didn't you start this workshop talking about how we need to account for the uncertainties on $x$ as well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Uncertainties on both axes\n",
    "\n",
    "To also account for the uncertainties on $x$, we need do define a new likelihood function.  We've gone through a lot of statistics already, so, for the sake of time, we'll breeze through this.  A more complete derivation is included in the Hogg, Bovy & Lang paper. \n",
    "\n",
    "If we assume the uncertainties are Gaussian (as we are doing), we can write the probability for obtaining a measurement, \n",
    "\n",
    "$$ Z_i = \\left[ {\\begin{array}{c}\n",
    "        x_i \\\\\n",
    "        y_i \\\\\n",
    "        \\end{array}\n",
    "       }\n",
    "       \\right] \n",
    "$$\n",
    "\n",
    "when the \"true value\" (the value you would have this data point if it had been observed with negligible noise) is:\n",
    "\n",
    "$$ Z = \\left[ {\\begin{array}{c}\n",
    "        x \\\\\n",
    "        y \\\\\n",
    "        \\end{array}\n",
    "       }\n",
    "       \\right] \n",
    "$$\n",
    "\n",
    "as \n",
    "\n",
    "$$ p\\left(x_i, y_i\\, |\\, S_i, x,y\\right) = \\frac{1}{2\\pi \\sqrt{\\det(S_i)}} \\exp\\left(-\\frac{1}{2}\\left[Z_i - Z\\right]^T S_i^{-1} \\left[Z_i - Z\\right]\\right).$$\n",
    "\n",
    "But how do we use this to fit a line?  (What should we use for Z?)  The \"trick\" is to project the 2D uncertainties onto the sub-space that is orthogonal to the line and to evaluate the projected displacements.  This will involve a change in variables from $(m, b)$ to $(\\theta, b_\\perp)$, as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta &= \\arctan m \\\\\n",
    "b_\\perp &= b \\cos \\theta \\\\\n",
    "\\hat{v} &= \\left[ {\\begin{array}{c}\n",
    "        -\\sin \\theta \\\\\n",
    "        \\cos \\theta \\\\\n",
    "        \\end{array}\n",
    "       }\n",
    "       \\right] \\\\\n",
    "\\Delta_i &= \\hat{v}\\, Z_i - b_\\perp \\\\\n",
    "\\Sigma_i^2 &= \\hat{v}^T\\, S_i\\, \\hat{v} \\\\\n",
    "\\ln \\mathscr{L} &= K - \\sum_{i=1}^{N} \\frac{\\Delta_i^2}{2\\Sigma_i^2} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In short, $\\Delta_i$ is the orthogonal displacement of each data point $(x_i, y_i)$from the line, and $\\Sigma^2$ is the orthogonal variance (by projecting the covariance matrix $S_i$, which we defined previously)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Whew!  Let's see that in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def lnlike2(theta, x, y, xerr, yerr, rhoxy):\n",
    "    th, bperp = theta\n",
    "    lnl = 0.\n",
    "    v = np.array([[-np.sin(th)], [np.cos(th)]])\n",
    "    vT = v.transpose()\n",
    "    for (xx, yy, sx, sy, rxy) in zip(x, y, xerr, yerr, rhoxy):\n",
    "        S = np.array([[ sx**2, rxy*sx*sy],\n",
    "                    [rxy*sx*sy, sy**2.]]) \n",
    "        Z = np.array([[xx],[yy]])\n",
    "        Delta = vT @ Z - bperp\n",
    "        Sigma2 = vT @ S @ v\n",
    "        lnl -= Delta**2. / (2. * Sigma2)      #this ignores K (a constant), which doesn't change the likelihood maximum\n",
    "        \n",
    "    return lnl#.squeeze()\n",
    "\n",
    "def lnprior2(theta):\n",
    "    th, bperp = theta\n",
    "    if ( (0 < np.abs(th) < 2.*np.pi) and (-200 < bperp < 200)):\n",
    "        return 0.0\n",
    "    return -np.inf\n",
    "\n",
    "def lnprob2(theta, x, y, xerr, yerr, rhoxy):\n",
    "    lnp = lnprior2(theta)\n",
    "    lnl = lnlike2(theta,  x, y, xerr, yerr, rhoxy)\n",
    "    if (not np.isfinite(lnp) or not np.isfinite(lnl)):\n",
    "        return -np.inf\n",
    "    return lnp + lnl\n",
    "\n",
    "def thFromM(m):\n",
    "    return np.arctan(m)\n",
    "def bpFromBth(b, th):\n",
    "    return b * np.cos(th)\n",
    "def mFromTh(th):\n",
    "    return np.tan(th)\n",
    "def bFromThBp(th, bp):\n",
    "    return bp/np.cos(th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise:\n",
    "\n",
    "Run this probability distribution through *emcee*, then print and plot the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Hint: find the related cell(s) above; copy the contents and paste it here.  Then make any necessary edits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# set a \"burn-in\" limit\n",
    "nburn = 200\n",
    "th_samples = np.array(sampler.chain[:,nburn:,0]).flatten()\n",
    "bp_samples = np.array(sampler.chain[:,nburn:,1]).flatten()\n",
    "m_samples = mFromTh(th_samples)\n",
    "b_samples = bFromThBp(th_samples, bp_samples)\n",
    "\n",
    "#print the results taking 1-sigma confidence intervals\n",
    "samples = np.vstack([m_samples, b_samples]).T\n",
    "m_mcmc, b_mcmc = map(lambda v: (v[1], v[2]-v[1], v[1]-v[0]), zip(*np.percentile(samples, [16, 50, 84], axis=0)))\n",
    "print(\"emcee results with 1-sigma uncertainties\\n \\\n",
    "      m = {:.4f} +{:.4f} -{:.4f}\\n \\\n",
    "      b = {:.4f} +{:.4f} -{:.4f}\\n\" \\\n",
    "      .format(m_mcmc[0], m_mcmc[1], m_mcmc[2], b_mcmc[0], b_mcmc[1], b_mcmc[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "paramsNames = [r'$\\theta$', r'$b_\\perp$']\n",
    "\n",
    "#chain plot\n",
    "axC = plotChains(sampler, nburn, paramsNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#corner plot\n",
    "makeCorner(sampler, nburn, paramsNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Adam's plots\n",
    "axA = prettyPlot(obs_x, obs_y, sigma_x, sigma_y, rho_xy, m_samples, b_samples, m_mcmc[0], b_mcmc[0], nburn, ndraw = 20)\n",
    "axA[1].set_xlim(0,300)\n",
    "axA[1].set_ylim(0,700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So far we haven't really showcased the power of Bayesian model fitting.  One example of when you would really want to use a Baysian tool is when you have model parameters that you want to *marginalize over*.  The following examples dive into this. \n",
    "\n",
    "We will first increase the scatter of our data by adding a few outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Instrinsic Scatter\n",
    "\n",
    "Let's assume that the data is still best described by a line, and we think the best fit solution is what we found above (without the extra scatter).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "obs_x = np.append(obs_x, np.array([201., 244., 287.]))\n",
    "obs_y = np.append(obs_y, np.array([592., 401., 402.]))\n",
    "sigma_x = np.append(sigma_x, np.array([9., 4., 7.])) \n",
    "sigma_y = np.append(sigma_y, np.array([61., 25., 15.]))\n",
    "rho_xy = np.append(rho_xy, np.array([-0.84, 0.31, -0.27]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise:\n",
    "\n",
    "Use our previous likelihood function with scipy.minimize to fit these data.  Print and plot the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Hint: find the related cell(s) above; copy the contents and paste it here.  Then make any necessary edits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The fit is different, clearly pulled by the outliers.  Both fit parameters are $>3\\sigma$ away from what we found previously.   Also, notice that (at least on my notebook) the minimize function is giving an ominous warning.  (I would not want to publish the result of that fit!)\n",
    "\n",
    "But we can deal with this using a slightly different likelihood function, and using *emcee*.  We will introduce one additional variable, $V$ : Gaussian variance orthogonal to the line.  Hogg et al. give us the likelihood:\n",
    "\n",
    "$$\n",
    "\\ln \\mathscr{L} = K - \\sum_{i=1}^{N}\\frac{1}{2}\\ln\\left(\\Sigma_i^2 + V\\right) - \\sum_{i=1}^{N} \\frac{\\Delta_i^2}{2\\left[\\Sigma_i^2 + V\\right]} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise:\n",
    "\n",
    "Code this up into a new likelihood, prior and posterior probability distribution.  Run this through *emcee*.  Print and plot the results.  In your plot that includes the fit line(s), also show the \"$1\\sigma$\" range in the scatter. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Hint: find the right cell above; copy the contents and paste it here.  Then make any necessary edits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "nwalkers = 100\n",
    "nfac = [1e-2, 1e-2, 1.]\n",
    "th = thFromM(mFreq)\n",
    "bp = bpFromBth(bFreq, th)\n",
    "guess0 = np.array([th, bp, 30.])\n",
    "ndim = len(guess0)\n",
    "pos = [guess0 + nfac * np.random.randn(ndim) for i in range(nwalkers)]\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprobS, args=(obs_x, obs_y, sigma_x, sigma_y, rho_xy), threads = ncores)\n",
    "\n",
    "nsamples = 2000\n",
    "foo = sampler.run_mcmc(pos, nsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# set a \"burn-in\" limit\n",
    "nburn = 200\n",
    "th_samples = np.array(sampler.chain[:,nburn:,0]).flatten()\n",
    "bp_samples = np.array(sampler.chain[:,nburn:,1]).flatten()\n",
    "s_samples = np.array(sampler.chain[:,nburn:,2]).flatten()\n",
    "m_samples = mFromTh(th_samples)\n",
    "b_samples = bFromThBp(th_samples, bp_samples)\n",
    "\n",
    "#print the results taking 1-sigma confidence intervals\n",
    "samples = np.vstack([m_samples, b_samples, s_samples]).T\n",
    "m_mcmc, b_mcmc, s_mcmc = map(lambda v: (v[1], v[2]-v[1], v[1]-v[0]), zip(*np.percentile(samples, [16, 50, 84], axis=0)))\n",
    "print(\"emcee results with 1-sigma uncertainties\\n \\\n",
    "      m = {:.4f} +{:.4f} -{:.4f}\\n \\\n",
    "      b = {:.4f} +{:.4f} -{:.4f}\\n \\\n",
    "      s = {:.4f} +{:.4f} -{:.4f}\\n\" \\\n",
    "      .format(m_mcmc[0], m_mcmc[1], m_mcmc[2], b_mcmc[0], b_mcmc[1], b_mcmc[2], s_mcmc[0], s_mcmc[1], s_mcmc[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "paramsNames = [r'$\\theta$', r'$b_\\perp$', r'$\\sqrt{V}$']\n",
    "\n",
    "#chain plot\n",
    "axC = plotChains(sampler, nburn, paramsNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#corner plot\n",
    "makeCorner(sampler, nburn, paramsNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Adam's plots\n",
    "fac = 10.\n",
    "axA = prettyPlot(obs_x, obs_y, sigma_x, sigma_y, rho_xy, m_samples, b_samples, m_mcmc[0], b_mcmc[0], nburn, ndraw = 0, \\\n",
    "                 extent = [m_mcmc[0] - fac*m_mcmc[2], m_mcmc[0] + fac*m_mcmc[1], b_mcmc[0] - fac*b_mcmc[2], b_mcmc[0] + fac*b_mcmc[1]])\n",
    "axA[0].set_xlim(m_mcmc[0] - fac*m_mcmc[2], m_mcmc[0] + fac*m_mcmc[1])\n",
    "axA[0].set_ylim(b_mcmc[0] - fac*b_mcmc[2], b_mcmc[0] + fac*b_mcmc[1])\n",
    "axA[1].set_xlim(0,300)\n",
    "axA[1].set_ylim(0,700)\n",
    "\n",
    "smax = s_mcmc[0] + s_mcmc[1]\n",
    "x0 = -b_mcmc[0]/m_mcmc[0] #x intercept\n",
    "beta = np.arctan2(x0, b_mcmc[0]) #angle between line and y axis\n",
    "alpha = np.pi/2. - beta #angle between perpendicular offset (s) and line\n",
    "d = smax / np.cos(alpha) #distance to perpendicular offset in y direction\n",
    "axA[1].plot([-1e10,1e10], [d + b_mcmc[0] + -1e10*m_mcmc[0], d + b_mcmc[0] + 1e10*m_mcmc[0]], ':k')\n",
    "axA[1].plot([-1e10,1e10], [-d + b_mcmc[0] + -1e10*m_mcmc[0], -d + b_mcmc[0] + 1e10*m_mcmc[0]], ':k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pruning Outliers\n",
    "\n",
    "We will now append an additional outlier to our data.\n",
    "\n",
    "Again, let's assume that the data is still best described by a line, and we think the best fit solution is what we found above (without the extra scatter or outliers). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "obs_x = np.append(obs_x, 47.)\n",
    "obs_y = np.append(obs_y, 583.)\n",
    "sigma_x = np.append(sigma_x, 11.) \n",
    "sigma_y = np.append(sigma_y, 38.)\n",
    "rho_xy = np.append(rho_xy, 0.64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise:\n",
    "\n",
    "Use our previous likelihood function with scipy.minimize to fit these data.  Print and plot the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Hint: find the related cell(s) above; copy the contents and paste it here.  Then make any necessary edits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Clearly this result is different from what we found before, and again we see the ominous warning from scipy.\n",
    "\n",
    "Given just the data, and thus without any direct knowledge which observations are \"outliers\", how could we verify this fit and deal with what we may think \"by eye\" are outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An ad hoc solution would be to try some type of sigma clipping algorithm.  This is very popular, but also rather subjective.  We don't want to introduce unnecesary biases into our fit.  So we want to ***avoid sigma clipping***.  \n",
    "\n",
    "Instead we will alter the likelihood function to describe a gaussian mixture model where some of the data that we have observed come from a distribution described by a line and the remaining observations come from a gaussian model. Ultimately, we don't care about which points are outliers, we are primarily interested in determining the values of $m$ and $b$. This is where we will truly take advantage of the Bayesian framework that we have been developing as we are going to *marginalize* over the outlier parameters to determine confidence regions on the values of $m$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With math: Recall our likelihood function that describes the fit line with uncertainties on both axes:\n",
    "\n",
    "$$\n",
    "\\ln \\mathscr{L} = K - \\sum_{i=1}^{N} \\frac{\\Delta_i^2}{2\\Sigma_i^2} \n",
    "$$\n",
    "\n",
    "This is the \"good\" model that we care about.  Let's assume that some fraction of our data, $p_g$, comes from this model.  For this step, we need the constant, $K$, to be defined explicitly. So let's be a bit more thorough:\n",
    "\n",
    "$$\n",
    "\\mathscr{L}_g = \\sum_{i=1}^{N} \\left( \\frac{p_g}{\\sqrt{2\\pi\\Sigma_i^2}} exp\\left(  - \\frac{\\Delta_i^2}{2\\Sigma_i^2}\\right) \\right)\n",
    "$$ \n",
    "\n",
    "We now assume that the remaining fraction, $1-p_g$, of our data comes from a \"bad\" model that we *don't* care about, but is described by some variance, $V_b$, and mean value, $\\Delta_b$, relative to the line:\n",
    "\n",
    "$$\n",
    "\\mathscr{L}_b = \\sum_{i=1}^{N} \\left( \\frac{1 - p_g}{\\sqrt{2\\pi\\left(\\Sigma_i^2 + V_b\\right)}} exp\\left(  - \\frac{\\left(\\Delta_i - \\Delta_b\\right)^2}{2\\left(\\Sigma_i^2 + V_b\\right)}\\right) \\right)\n",
    "$$\n",
    "\n",
    "The total log likelihood is the sum of these two log likelihoods written above\"\n",
    "\n",
    "\n",
    "$$\n",
    "\\ln \\mathscr{L} = \\ln \\left(\\mathscr{L}_g + \\mathscr{L}_b \\right)\n",
    "$$\n",
    "\n",
    "In principle we could also use this model to evaluate if any given data point is drawn from the outlier distribution (\"bad\") or the line (\"good\").  There's an interesting discussion on this on [Dan Foreman-Mackey's blog](http://dfm.io/posts/mixture-models/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#writing things slightly differently here so that I can return both \"blobs\" from emcee\n",
    "def lnlikeOg(theta, x, y, xerr, yerr, rhoxy):\n",
    "    th, bperp, _, _, _ = theta\n",
    "    lnlg = np.array([])\n",
    "    v = np.array([[-np.sin(th)], [np.cos(th)]])\n",
    "    vT = v.transpose()\n",
    "    for (xx, yy, sx, sy, rxy) in zip(x, y, xerr, yerr, rhoxy):\n",
    "        S = np.array([[ sx**2, rxy*sx*sy],\n",
    "                    [rxy*sx*sy, sy**2.]]) \n",
    "        Z = np.array([[xx],[yy]])\n",
    "        Delta = vT @ Z - bperp\n",
    "        Sigma2 = vT @ S @ v\n",
    "        \n",
    "        lnlg = np.append(lnlg, -np.log(np.sqrt(2*np.pi*Sigma2)) - Delta**2/(2*Sigma2))\n",
    "\n",
    "    return lnlg\n",
    "\n",
    "def lnlikeOb(theta, x, y, xerr, yerr, rhoxy):\n",
    "    th, bperp, p_g, sigma_b, delta_b = theta\n",
    "    sigma_b = np.clip(sigma_b, 0, 2000)         # prevent errors in likelihood eval\n",
    "    lnlb = np.array([])\n",
    "    v = np.array([[-np.sin(th)], [np.cos(th)]])\n",
    "    vT = v.transpose()\n",
    "    for (xx, yy, sx, sy, rxy) in zip(x, y, xerr, yerr, rhoxy):\n",
    "        S = np.array([[ sx**2, rxy*sx*sy],\n",
    "                    [rxy*sx*sy, sy**2.]]) \n",
    "        Z = np.array([[xx],[yy]])\n",
    "        Delta = vT @ Z - bperp\n",
    "        Sigma2 = vT @ S @ v\n",
    "        \n",
    "        lnlb = np.append(lnlb, -np.log(np.sqrt(2*np.pi*(Sigma2 + sigma_b**2.))) - (Delta - delta_b)**2/(2*(Sigma2 + sigma_b**2.)))        \n",
    "\n",
    "    return lnlb\n",
    "\n",
    "#priors\n",
    "def lnpriorO(theta):\n",
    "    th, bperp, p_g, sigma_b, delta_b = theta\n",
    "    if ( (0 < np.abs(th) < 2.*np.pi) and (-200 < bperp < 200) and (0 <= sigma_b < 2000)\n",
    "        and (0 < p_g < 1) and (-2000 < delta_b < 2000)):\n",
    "        return 0.0\n",
    "    return -np.inf\n",
    "\n",
    "\n",
    "def lnprobO(theta, x, y, xerr, yerr, rhoxy):\n",
    "    th, bperp, p_g, sigma_b, delta_b = theta\n",
    "\n",
    "    lnp = lnpriorO(theta)  \n",
    "    p_g = np.clip(p_g, 1e-20, 1 - 1e-20)         # prevent errors in likelihood eval\n",
    "\n",
    "    lnlg = lnlikeOg(theta,  x, y, xerr, yerr, rhoxy) \n",
    "    arg1 = lnlg + np.log(p_g)\n",
    "                           \n",
    "    lnlb = lnlikeOb(theta,  x, y, xerr, yerr, rhoxy)\n",
    "    arg2 = lnlb + np.log(1. - p_g)\n",
    "                              \n",
    "    lnl = np.sum(np.logaddexp(arg1, arg2))\n",
    "                      \n",
    "    if (not np.isfinite(lnp) or not np.isfinite(lnl)):\n",
    "        return -np.inf, None\n",
    "    \n",
    "    # We're using emcee's \"blobs\" feature in order to keep track of the\n",
    "    # \"good\" and \"bad\" likelihoods for reasons that will become\n",
    "    # clear soon.\n",
    "    \n",
    "    return lnp + lnl, (arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise:\n",
    "\n",
    "Run this probability distribution through *emcee*, then print and plot the results. (Hint, you may need to increase the number of samples and the burn-in limit because we are now fitting to more parameters.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Hint: find the related cell(s) above; copy the contents and paste it here.  Then make any necessary edits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Now let's work with real data\n",
    "\n",
    "I pulled Table 1 from [this paper on the \"M-$\\sigma$\" relation](http://adsabs.harvard.edu/abs/2009ApJ...698..198G)\n",
    "\n",
    "I will use *pandas* to read in and reformat the data, but feel free to do this instead with your own favorite method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#data from : http://adsabs.harvard.edu/abs/2009ApJ...698..198G\n",
    "#They find log(MBH/M) = Î± + Î² log(Ïƒ/200 km sâˆ’1) with (Î±,Î²,0) = (8.12 Â± 0.08, 4.24 Â± 0.41, 0.44 Â± 0.06) \n",
    "#But they include some with upper limits that I don't have here\n",
    "mSigma = pd.read_csv('apj303542t1_ascii.txt', quotechar='\"', sep='\\t')\n",
    "#print(mSigma.columns.values)\n",
    "display(mSigma)\n",
    "\n",
    "def fixSci(series):\n",
    "    x = series.str.split(' ', expand = True)\n",
    "    x2 = x[:][2].str.split('^', expand = True)\n",
    "    return(x[:][0].astype(\"float\") * 10.**x2[:][1].astype(\"float\"))\n",
    "\n",
    "def fixSig(series):\n",
    "    x = series.str.split(' ', expand = True)\n",
    "    return(x[:][0].astype(\"float\"))\n",
    "def fixSige(series):\n",
    "    x = series.str.split(' ', expand = True)\n",
    "    x2 = x[:][2].str.split('^', expand = True)\n",
    "    return(x2[:][0].astype(\"float\"))\n",
    "\n",
    "\n",
    "obs_x = np.array(fixSig(mSigma['sigma_e (km s^-1)']))\n",
    "obs_lx = np.log10(obs_x)\n",
    "sigma_lx = 1./(obs_x * np.log(10.)) * np.array(fixSige(mSigma['sigma_e (km s^-1)']))\n",
    "\n",
    "obs_y = np.array(fixSci(mSigma['M_BH (M_sun)']))\n",
    "obs_ly = np.log10(obs_y)\n",
    "\n",
    "#this is probably not what we would want for publication, but we can first try to take a mean uncertainty in x\n",
    "y_low = np.array(fixSci(mSigma['M_low (M_sun)']))\n",
    "y_high = np.array(fixSci(mSigma['M_high (M_sun)']))\n",
    "ye_low = obs_y - y_low\n",
    "ye_high = y_high - obs_y\n",
    "ye = np.array([ (y1 + y2)/2. for (y1, y2) in zip(ye_low, ye_high)])\n",
    "sigma_ly = 1./(obs_y * np.log(10.)) * ye\n",
    "\n",
    "rho_lxy = np.full_like(obs_x, 0.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise:\n",
    "\n",
    "Plot these data (\"$x$\" is $\\log_{10}\\sigma$,  and \"$y$\" is $\\log_{10} M$).  Then run the data through *emcee*, using the likelihood that accounts for intrinsic scatter.  Print and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Hint: find the related cell(s) above; copy the contents and paste it here.  Then make any necessary edits."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
